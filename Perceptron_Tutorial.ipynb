{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f66b8YJk4rGZ"
      },
      "outputs": [],
      "source": [
        "#NumPy for Matrix Computation\n",
        "import numpy as np\n",
        "\n",
        "#Train Test split for shuffling the data and splits it \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the IRIS Dataset for classification. \n",
        "IRIS dataset contains 3 different classes but we are only doing binary classification in this case.\n",
        "\n",
        "The next steps basically select only samples from class 1 and 2; then split them into training and test set\n",
        "\n",
        "> IRIS dataset is simple and thus could be easily read using NumPy.\n",
        "\n",
        "> In practice, dataset can have irregular fields, missing values, non-numeric values, etc. which should be handled using pandas.\n",
        "\n"
      ],
      "metadata": {
        "id": "8YTDkzW9ECvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset into x and y sets\n",
        "X, y = load_iris(return_X_y=True)"
      ],
      "metadata": {
        "id": "C_XbuqHC5A6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Input Feature shape: ', X.shape)\n",
        "print ('Output Shape: ', y.shape)\n",
        "print ('First 10 columns:')\n",
        "\n",
        "\n",
        "print ('         X            ')\n",
        "print (X[:10])\n",
        "\n",
        "print ('y: ', y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE8fgiRT5H7m",
        "outputId": "e0eabfce-adb4-4c9c-c4c3-922eef7f2a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Feature shape:  (150, 4)\n",
            "Output Shape:  (150,)\n",
            "First 10 columns:\n",
            "         X            \n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]]\n",
            "y:  [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out how many classes in y:\n",
        "print (\"Using numpy: \", np.unique(y))\n",
        "print (\"Using set: \", set(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBRPoYpmEY3A",
        "outputId": "f1abd1dc-5801-4dc9-ef2f-104b3632f937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using numpy:  [0 1 2]\n",
            "Using set:  {0, 1, 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[50:],y[50:],test_size=0.2) #We only use the last two classes\n"
      ],
      "metadata": {
        "id": "heJinitTYjcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('The data should contains only class 1 and 2 from now')\n",
        "print (set(y_train))\n",
        "print (set(y_test))\n",
        "\n",
        "#Label Mapping\n",
        "y_train[y_train == 1] = 1\n",
        "y_train[y_train != 1] = -1\n",
        "y_test[y_test == 1] = 1\n",
        "y_test[y_test != 1] = -1\n",
        "\n",
        "print ('After label mapping to +1 and -1')\n",
        "print (set(y_train))\n",
        "print (set(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8l2JGMgfkO8",
        "outputId": "e2a6faac-2368-4285-d958-319222025c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data should contains only class 1 and 2 from now\n",
            "{1, 2}\n",
            "{1, 2}\n",
            "After label mapping to +1 and -1\n",
            "{1, -1}\n",
            "{1, -1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training and testing"
      ],
      "metadata": {
        "id": "INkavSwo9FyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
        "pct=Perceptron()\n",
        "pct.fit(X_train,y_train)\n",
        "\n",
        "#Pass in the test features into the trained model\n",
        "pred_pct=pct.predict(X_test)\n",
        "print (\"Confusion Matrix: \", confusion_matrix(y_test,pred_pct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STwivmUCcMM1",
        "outputId": "55992b42-7979-410c-b21a-3f4c527e9f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:  [[11  0]\n",
            " [ 6  3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pct=Perceptron(max_iter=5000, verbose=1)\n",
        "pct.fit(X_train,y_train)\n",
        "\n",
        "#Pass in the test features into the trained model\n",
        "pred_pct=pct.predict(X_test)\n",
        "print (\"Confusion Matrix: \", confusion_matrix(y_test,pred_pct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgRK1rq3djan",
        "outputId": "3e5a91a2-fe39-4005-c562-3064a7e055ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 12.50, NNZs: 4, Bias: 3.000000, T: 80, Avg. loss: 13.857875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 26.42, NNZs: 4, Bias: 5.000000, T: 160, Avg. loss: 14.786625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 31.47, NNZs: 4, Bias: 6.000000, T: 240, Avg. loss: 6.115250\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 35.55, NNZs: 4, Bias: 7.000000, T: 320, Avg. loss: 5.200500\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 41.56, NNZs: 4, Bias: 8.000000, T: 400, Avg. loss: 6.921375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 45.13, NNZs: 4, Bias: 9.000000, T: 480, Avg. loss: 3.242125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 45.56, NNZs: 4, Bias: 9.000000, T: 560, Avg. loss: 2.668750\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 47.38, NNZs: 4, Bias: 9.000000, T: 640, Avg. loss: 2.412000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 50.15, NNZs: 4, Bias: 10.000000, T: 720, Avg. loss: 3.965875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 56.60, NNZs: 4, Bias: 12.000000, T: 800, Avg. loss: 6.060375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 60.90, NNZs: 4, Bias: 13.000000, T: 880, Avg. loss: 4.420375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 62.79, NNZs: 4, Bias: 14.000000, T: 960, Avg. loss: 3.090000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 64.61, NNZs: 4, Bias: 14.000000, T: 1040, Avg. loss: 4.360750\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 13 epochs took 0.00 seconds\n",
            "Confusion Matrix:  [[11  0]\n",
            " [ 6  3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question in Programming Assignment 1: why doesn't the training reach 5000 epochs? "
      ],
      "metadata": {
        "id": "BbRCbPrQeQWR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSAjYqQzVpK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
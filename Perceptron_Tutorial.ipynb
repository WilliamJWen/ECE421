{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f66b8YJk4rGZ"
      },
      "outputs": [],
      "source": [
        "#NumPy for Matrix Computation\n",
        "import numpy as np\n",
        "\n",
        "#Train Test split for shuffling the data and splits it\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the IRIS Dataset for classification.\n",
        "IRIS dataset contains 3 different classes but we are only doing binary classification in this case.\n",
        "\n",
        "The next steps basically select only samples from class 1 and 2; then split them into training and test set\n",
        "\n",
        "> IRIS dataset is simple and thus could be easily read using NumPy.\n",
        "\n",
        "> In practice, dataset can have irregular fields, missing values, non-numeric values, etc. which should be handled using pandas.\n",
        "\n"
      ],
      "metadata": {
        "id": "8YTDkzW9ECvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset into x and y sets\n",
        "X, y = load_iris(return_X_y=True)"
      ],
      "metadata": {
        "id": "C_XbuqHC5A6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Input Feature shape: ', X.shape)\n",
        "print ('Output Shape: ', y.shape)\n",
        "print ('First 10 columns:')\n",
        "\n",
        "\n",
        "print ('         X            ')\n",
        "print (X[:10])\n",
        "\n",
        "print ('y: ', y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE8fgiRT5H7m",
        "outputId": "a7d299ad-34d0-432c-951a-659f873133aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Feature shape:  (150, 4)\n",
            "Output Shape:  (150,)\n",
            "First 10 columns:\n",
            "         X            \n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]]\n",
            "y:  [0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find out how many classes in y:\n",
        "print (\"Using numpy: \", np.unique(y))\n",
        "print (\"Using set: \", set(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBRPoYpmEY3A",
        "outputId": "e98ac6a0-a313-4748-d538-44ba92271848"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using numpy:  [0 1 2]\n",
            "Using set:  {np.int64(0), np.int64(1), np.int64(2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[50:],y[50:],test_size=0.2) #We only use the last two classes\n",
        "\n",
        "# first 50 is all 0, second 50 is all 1, third 50 is all 2\n",
        "# throwing out first 50\n"
      ],
      "metadata": {
        "id": "heJinitTYjcA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('The data should contains only class 1 and 2 from now')\n",
        "print (set(y_train))\n",
        "print (set(y_test))\n",
        "\n",
        "#Label Mapping\n",
        "y_train[y_train == 1] = 1\n",
        "y_train[y_train != 1] = -1\n",
        "y_test[y_test == 1] = 1\n",
        "y_test[y_test != 1] = -1\n",
        "\n",
        "print ('After label mapping to +1 and -1')\n",
        "print (set(y_train))\n",
        "print (set(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8l2JGMgfkO8",
        "outputId": "0483fc4d-ee6f-4dda-eeee-6e28efa56e5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data should contains only class 1 and 2 from now\n",
            "{np.int64(1), np.int64(2)}\n",
            "{np.int64(1), np.int64(2)}\n",
            "After label mapping to +1 and -1\n",
            "{np.int64(1), np.int64(-1)}\n",
            "{np.int64(1), np.int64(-1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training and testing"
      ],
      "metadata": {
        "id": "INkavSwo9FyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
        "pct=Perceptron()\n",
        "pct.fit(X_train,y_train)\n",
        "\n",
        "#Pass in the test features into the trained model\n",
        "pred_pct=pct.predict(X_test)\n",
        "print (\"Confusion Matrix: \", confusion_matrix(y_test,pred_pct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STwivmUCcMM1",
        "outputId": "34d74585-0029-4048-c5e5-38a5acdd2bc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:  [[ 8  0]\n",
            " [ 2 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pct=Perceptron(max_iter=5000, verbose=1)\n",
        "pct.fit(X_train,y_train)\n",
        "\n",
        "#Pass in the test features into the trained model\n",
        "pred_pct=pct.predict(X_test)\n",
        "print (\"Confusion Matrix: \", confusion_matrix(y_test,pred_pct))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgRK1rq3djan",
        "outputId": "cab8e639-4c68-4380-c7b9-641b8482024c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 14.07, NNZs: 4, Bias: 3.000000, T: 80, Avg. loss: 15.186000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 23.57, NNZs: 4, Bias: 5.000000, T: 160, Avg. loss: 11.781375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 30.43, NNZs: 4, Bias: 7.000000, T: 240, Avg. loss: 7.877375\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 33.33, NNZs: 4, Bias: 7.000000, T: 320, Avg. loss: 4.634125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 44.24, NNZs: 4, Bias: 9.000000, T: 400, Avg. loss: 9.127000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 46.17, NNZs: 4, Bias: 11.000000, T: 480, Avg. loss: 3.183625\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 54.79, NNZs: 4, Bias: 13.000000, T: 560, Avg. loss: 7.485125\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 57.62, NNZs: 4, Bias: 14.000000, T: 640, Avg. loss: 4.286875\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 61.90, NNZs: 4, Bias: 14.000000, T: 720, Avg. loss: 5.203250\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 66.85, NNZs: 4, Bias: 17.000000, T: 800, Avg. loss: 4.515000\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 70.62, NNZs: 4, Bias: 17.000000, T: 880, Avg. loss: 4.352375\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 11 epochs took 0.00 seconds\n",
            "Confusion Matrix:  [[ 8  0]\n",
            " [ 2 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question in Programming Assignment 1: why doesn't the training reach 5000 epochs?"
      ],
      "metadata": {
        "id": "BbRCbPrQeQWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pct=Perceptron(max_iter=5000, verbose=1) has a default parameter, when the loss converges, it stopped training."
      ],
      "metadata": {
        "id": "rSAjYqQzVpK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}